# ğŸ—ï¸ Imovelweb Scraper â€” Manual de Uso

## ğŸ“ Estrutura dos Arquivos

- `extract_data.py`: cÃ³digo principal do scraper.
- `cookies.json`: cookies salvos automaticamente ao colar o cabeÃ§alho `Cookie`.
- `listings.json`: todos os anÃºncios coletados, **acumulados** (sem sobrescrever).
- `scraper_state.json`: salva a Ãºltima pÃ¡gina raspada, total de anÃºncios e data/hora.

---

## â–¶ï¸ Como Executar o Scraper

```bash
py extract_data.py --url "https://www.imovelweb.com.br/imoveis-venda-sao-bernardo-do-campo-sp-pagina-1.html"
```

---

## ğŸ’¡ Funcionamento

1. A URL determina automaticamente a cidade (`sao-bernardo-do-campo`) e a operaÃ§Ã£o (`venda` ou `aluguel`).
2. O scraper acessa a **API interna** do Imovelweb e coleta os anÃºncios.
3. Se for a primeira execuÃ§Ã£o, comeÃ§a na pÃ¡gina 1. Caso contrÃ¡rio, retoma da **Ãºltima pÃ¡gina salva**.
4. Os anÃºncios sÃ£o **acumulados** no `listings.json` (mesmo que sejam repetidos).
5. ApÃ³s cada pÃ¡gina:
   - Salva os anÃºncios da pÃ¡gina no arquivo.
   - Atualiza `scraper_state.json`.

---

## ğŸª Sobre Cookies

Se ocorrer o erro `403 (acesso negado)`, serÃ¡ solicitado o cabeÃ§alho `Cookie`.

### Como obter os cookies:
1. Acesse a **Ãºltima pÃ¡gina visitada** do site (ex: `https://www.imovelweb.com.br/imoveis-venda-sao-bernardo-do-campo-sp-pagina-20.html`).
2. Abra o DevTools (`F12`) â†’ aba `Network` â†’ clique em qualquer request do tipo `XHR`.
3. Va atÃ© o cabeÃ§alho **Request Headers** â†’ copie **TODO o campo `Cookie`**.
4. Cole no terminal quando solicitado.

---

## âš™ï¸ ParÃ¢metros Opcionais

| ParÃ¢metro      | DescriÃ§Ã£o |
|----------------|-----------|
| `--output`     | Nome do arquivo de saÃ­da (padrÃ£o: `listings.json`) |
| `--cookies`    | Caminho para arquivo `.json` com cookies, ou string direta |

---

## ğŸ“Œ Exemplo com cookies via argumento:

```bash
py extract_data.py --url "URL" --cookies "cookie1=valor1; cookie2=valor2"
```

---

## â— ObservaÃ§Ãµes

- O scraper pausa 2 segundos entre cada pÃ¡gina para evitar bloqueios.
- Os anÃºncios sÃ£o salvos **completos ou incompletos**, para tratamento posterior.
- Se desejar **reiniciar o scraping**, basta excluir o arquivo `scraper_state.json`.

