{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "CÓDIGO R CONVERTIDO PARA PYTHON"
      ],
      "metadata": {
        "id": "StRA-yPU9v_j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwkibTSm9ufI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import geopandas as gpd\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import statsmodels.api as sm\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import gaussian_kde\n",
        "\n",
        "# Definir diretório de trabalho\n",
        "os.chdir(\"D:/OneDrive/09 - Mestrado/Mestrado UFABC/Dissertação/experimento/BAIXADA SANTISTA\")\n",
        "\n",
        "# Carregar dados\n",
        "grid = gpd.read_file(\"modelo_cel_rmbs_ap.gpkg\")\n",
        "points = gpd.read_file(\"ANUNCIOS_RMBS_SHP.shp\")\n",
        "\n",
        "# Renomear colunas\n",
        "points = points.rename(columns={\n",
        "    'fid': 'ID',\n",
        "    'field_1': 'Valor_venda',\n",
        "    'field_2': 'Valor_cond',\n",
        "    'field_3': 'End',\n",
        "    'field_4': 'Lat',\n",
        "    'field_5': 'Long',\n",
        "    'field_6': 'Area',\n",
        "    'field_7': 'Desc'\n",
        "})\n",
        "\n",
        "points['Valor_venda'] = pd.to_numeric(points['Valor_venda'].astype(str).str.replace(',', '.'), errors='coerce')\n",
        "\n",
        "# Criar colunas binárias no grid\n",
        "grid['tec_full'] = grid['TEC'].isin([\"1\", \"2\", \"3\", \"4\", \"5\", \"6\"]).astype(int)\n",
        "grid['t12346'] = grid['TEC'].isin([\"1\", \"2\", \"3\", \"4\", \"6\"]).astype(int)\n",
        "\n",
        "grid['tec_full'] = grid['tec_full'].fillna(0)\n",
        "grid['t12346'] = grid['t12346'].fillna(0)\n",
        "\n",
        "# Criar densidade de Kernel\n",
        "def kde_estimation(points, grid, bandwidth=500):\n",
        "    kde = gaussian_kde(points[['Long', 'Lat']].T, bw_method=bandwidth/1000)\n",
        "    grid['kde_value'] = kde(grid[['geometry']].apply(lambda x: [x.x, x.y], axis=1).tolist())\n",
        "    return grid\n",
        "\n",
        "grid = kde_estimation(points, grid)\n",
        "\n",
        "# Modelos de regressão logística\n",
        "X = grid[['kde_value']]\n",
        "y1 = grid['tec_full']\n",
        "y2 = grid['t12346']\n",
        "\n",
        "logit1 = sm.Logit(y1, sm.add_constant(X)).fit()\n",
        "logit2 = sm.Logit(y2, sm.add_constant(X)).fit()\n",
        "\n",
        "print(logit1.summary())\n",
        "print(logit2.summary())\n",
        "\n",
        "grid['model1'] = logit1.predict(sm.add_constant(X))\n",
        "grid['model2'] = logit2.predict(sm.add_constant(X))\n",
        "\n",
        "# Salvar resultados\n",
        "grid.to_file(\"0_GRADE_FINAL.gpkg\", driver=\"GPKG\")\n",
        "\n",
        "# Modelo Multinomial\n",
        "grid_precarious = grid[grid['TEC'] != \"0\"].copy()\n",
        "scaler = StandardScaler()\n",
        "grid_precarious[['densidade', 'TEC_Rev']] = scaler.fit_transform(grid_precarious[['kde_value', 'DEN_OCUP']])\n",
        "\n",
        "y_multinomial = grid_precarious['TEC']\n",
        "X_multinomial = grid_precarious[['densidade', 'TEC_Rev']]\n",
        "\n",
        "multi_model = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
        "multi_model.fit(X_multinomial, y_multinomial)\n",
        "\n",
        "grid_precarious['predito'] = multi_model.predict(X_multinomial)\n",
        "grid_precarious.to_file(\"grid_precarious_reg_multinomial.shp\", driver=\"ESRI Shapefile\")\n"
      ]
    }
  ]
}